{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep Learning on CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "*Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 13:57:26.804759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/Adit/codes/ml_101/handson-ml2-master/ml_env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "*Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let's use the first 5,000 images of the original training set as the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the callbacks we need and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time when we train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 20s 10ms/step - loss: 1.5682 - accuracy: 0.4366 - val_loss: 1.7614 - val_accuracy: 0.3766\n",
      "Epoch 2/100\n",
      "  12/1407 [..............................] - ETA: 13s - loss: 1.6323 - accuracy: 0.3828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Adit/codes/ml_101/handson-ml2-master/ml_env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5457 - accuracy: 0.4427 - val_loss: 1.7883 - val_accuracy: 0.3772\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5321 - accuracy: 0.4489 - val_loss: 1.7436 - val_accuracy: 0.3952\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5142 - accuracy: 0.4554 - val_loss: 1.6745 - val_accuracy: 0.4138\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4938 - accuracy: 0.4630 - val_loss: 1.6120 - val_accuracy: 0.4264\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4805 - accuracy: 0.4696 - val_loss: 1.6112 - val_accuracy: 0.4376\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4632 - accuracy: 0.4758 - val_loss: 1.6487 - val_accuracy: 0.4210\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4489 - accuracy: 0.4808 - val_loss: 1.5295 - val_accuracy: 0.4600\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4366 - accuracy: 0.4841 - val_loss: 1.5373 - val_accuracy: 0.4552\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4185 - accuracy: 0.4904 - val_loss: 1.6077 - val_accuracy: 0.4350\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4082 - accuracy: 0.4949 - val_loss: 1.5564 - val_accuracy: 0.4486\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3900 - accuracy: 0.5007 - val_loss: 1.5988 - val_accuracy: 0.4396\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3778 - accuracy: 0.5016 - val_loss: 1.5428 - val_accuracy: 0.4574\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3657 - accuracy: 0.5075 - val_loss: 1.5089 - val_accuracy: 0.4706\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3557 - accuracy: 0.5121 - val_loss: 1.6231 - val_accuracy: 0.4392\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3433 - accuracy: 0.5166 - val_loss: 1.5393 - val_accuracy: 0.4624\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3341 - accuracy: 0.5179 - val_loss: 1.5496 - val_accuracy: 0.4580\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3193 - accuracy: 0.5258 - val_loss: 1.5235 - val_accuracy: 0.4626\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3042 - accuracy: 0.5300 - val_loss: 1.5471 - val_accuracy: 0.4606\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2956 - accuracy: 0.5343 - val_loss: 1.5370 - val_accuracy: 0.4608\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2877 - accuracy: 0.5368 - val_loss: 1.5399 - val_accuracy: 0.4702\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2751 - accuracy: 0.5419 - val_loss: 1.5149 - val_accuracy: 0.4788\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2700 - accuracy: 0.5446 - val_loss: 1.5528 - val_accuracy: 0.4578\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2551 - accuracy: 0.5497 - val_loss: 1.5119 - val_accuracy: 0.4736\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2454 - accuracy: 0.5526 - val_loss: 1.4978 - val_accuracy: 0.4788\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2341 - accuracy: 0.5562 - val_loss: 1.5159 - val_accuracy: 0.4756\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2242 - accuracy: 0.5602 - val_loss: 1.4958 - val_accuracy: 0.4790\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2156 - accuracy: 0.5635 - val_loss: 1.5931 - val_accuracy: 0.4534\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2103 - accuracy: 0.5646 - val_loss: 1.4958 - val_accuracy: 0.4836\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1983 - accuracy: 0.5694 - val_loss: 1.6365 - val_accuracy: 0.4558\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1909 - accuracy: 0.5729 - val_loss: 1.6163 - val_accuracy: 0.4552\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1816 - accuracy: 0.5748 - val_loss: 1.5030 - val_accuracy: 0.4794\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1708 - accuracy: 0.5796 - val_loss: 1.5539 - val_accuracy: 0.4696\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1623 - accuracy: 0.5813 - val_loss: 1.5725 - val_accuracy: 0.4780\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1542 - accuracy: 0.5862 - val_loss: 1.5291 - val_accuracy: 0.4814\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1476 - accuracy: 0.5883 - val_loss: 1.5205 - val_accuracy: 0.4768\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1368 - accuracy: 0.5911 - val_loss: 1.5158 - val_accuracy: 0.4874\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1296 - accuracy: 0.5941 - val_loss: 1.5230 - val_accuracy: 0.4824\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1226 - accuracy: 0.5963 - val_loss: 1.5434 - val_accuracy: 0.4796\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1128 - accuracy: 0.5983 - val_loss: 1.5352 - val_accuracy: 0.4850\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1098 - accuracy: 0.6028 - val_loss: 1.5105 - val_accuracy: 0.4880\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.0987 - accuracy: 0.6041 - val_loss: 1.5320 - val_accuracy: 0.4846\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0929 - accuracy: 0.6058 - val_loss: 1.6112 - val_accuracy: 0.4646\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0833 - accuracy: 0.6112 - val_loss: 1.6336 - val_accuracy: 0.4680\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0758 - accuracy: 0.6126 - val_loss: 1.5590 - val_accuracy: 0.4892\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0680 - accuracy: 0.6147 - val_loss: 1.5484 - val_accuracy: 0.4900\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0619 - accuracy: 0.6188 - val_loss: 1.5734 - val_accuracy: 0.4902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x12bf3cd00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4958 - accuracy: 0.4790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.495756983757019, 0.4790000021457672]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the lowest validation loss gets about 47.6% accuracy on the validation set. It took 27 epochs to reach the lowest validation loss, with roughly 8 seconds per epoch on my laptop (without a GPU). Let's see if we can improve performance using Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "*Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "* I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.\n",
    "* I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
    "* I renamed the run directories to run_bn_* and the model file name to my_cifar10_bn_model.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 1.8377 - accuracy: 0.3408"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Adit/codes/ml_101/handson-ml2-master/ml_env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 39s 17ms/step - loss: 1.8377 - accuracy: 0.3408 - val_loss: 1.6608 - val_accuracy: 0.4044\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.6691 - accuracy: 0.4023 - val_loss: 1.5613 - val_accuracy: 0.4414\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.5984 - accuracy: 0.4330 - val_loss: 1.5350 - val_accuracy: 0.4504\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.5506 - accuracy: 0.4509 - val_loss: 1.5057 - val_accuracy: 0.4702\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.5077 - accuracy: 0.4654 - val_loss: 1.4454 - val_accuracy: 0.4942\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4737 - accuracy: 0.4781 - val_loss: 1.4214 - val_accuracy: 0.4968\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.4372 - accuracy: 0.4913 - val_loss: 1.4253 - val_accuracy: 0.4940\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.4104 - accuracy: 0.5022 - val_loss: 1.3836 - val_accuracy: 0.5134\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3848 - accuracy: 0.5107 - val_loss: 1.3879 - val_accuracy: 0.5094\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3603 - accuracy: 0.5200 - val_loss: 1.3463 - val_accuracy: 0.5216\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.3416 - accuracy: 0.5259 - val_loss: 1.3550 - val_accuracy: 0.5202\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3179 - accuracy: 0.5348 - val_loss: 1.3820 - val_accuracy: 0.5078\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.2957 - accuracy: 0.5415 - val_loss: 1.3606 - val_accuracy: 0.5164\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.2790 - accuracy: 0.5496 - val_loss: 1.3504 - val_accuracy: 0.5276\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.2571 - accuracy: 0.5569 - val_loss: 1.3767 - val_accuracy: 0.5212\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2524 - accuracy: 0.5576 - val_loss: 1.3635 - val_accuracy: 0.5278\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.2303 - accuracy: 0.5656 - val_loss: 1.3494 - val_accuracy: 0.5328\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.2141 - accuracy: 0.5733 - val_loss: 1.3524 - val_accuracy: 0.5290\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.2007 - accuracy: 0.5774 - val_loss: 1.3294 - val_accuracy: 0.5378\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.1863 - accuracy: 0.5803 - val_loss: 1.3669 - val_accuracy: 0.5312\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.1732 - accuracy: 0.5878 - val_loss: 1.3515 - val_accuracy: 0.5334\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.1583 - accuracy: 0.5900 - val_loss: 1.3548 - val_accuracy: 0.5256\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.1468 - accuracy: 0.5958 - val_loss: 1.3431 - val_accuracy: 0.5386\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.1336 - accuracy: 0.6012 - val_loss: 1.3107 - val_accuracy: 0.5506\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.1237 - accuracy: 0.6070 - val_loss: 1.3559 - val_accuracy: 0.5378\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.1094 - accuracy: 0.6099 - val_loss: 1.3437 - val_accuracy: 0.5318\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.0994 - accuracy: 0.6125 - val_loss: 1.3464 - val_accuracy: 0.5372\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0914 - accuracy: 0.6158 - val_loss: 1.3529 - val_accuracy: 0.5422\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0796 - accuracy: 0.6220 - val_loss: 1.3465 - val_accuracy: 0.5366\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0674 - accuracy: 0.6261 - val_loss: 1.3546 - val_accuracy: 0.5372\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.0545 - accuracy: 0.6306 - val_loss: 1.3646 - val_accuracy: 0.5404\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0455 - accuracy: 0.6348 - val_loss: 1.3613 - val_accuracy: 0.5398\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0312 - accuracy: 0.6346 - val_loss: 1.3661 - val_accuracy: 0.5434\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0301 - accuracy: 0.6387 - val_loss: 1.3425 - val_accuracy: 0.5484\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.0164 - accuracy: 0.6426 - val_loss: 1.3657 - val_accuracy: 0.5422\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0141 - accuracy: 0.6456 - val_loss: 1.3540 - val_accuracy: 0.5388\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.9969 - accuracy: 0.6490 - val_loss: 1.3843 - val_accuracy: 0.5342\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 0.9929 - accuracy: 0.6518 - val_loss: 1.3604 - val_accuracy: 0.5350\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9809 - accuracy: 0.6533 - val_loss: 1.3465 - val_accuracy: 0.5506\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 0.9706 - accuracy: 0.6606 - val_loss: 1.3874 - val_accuracy: 0.5364\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.9623 - accuracy: 0.6626 - val_loss: 1.3655 - val_accuracy: 0.5470\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.9540 - accuracy: 0.6645 - val_loss: 1.3620 - val_accuracy: 0.5416\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.9513 - accuracy: 0.6648 - val_loss: 1.3777 - val_accuracy: 0.5438\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 46s 33ms/step - loss: 0.9411 - accuracy: 0.6687 - val_loss: 1.3900 - val_accuracy: 0.5410\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.3107 - accuracy: 0.5506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.310655117034912, 0.550599992275238]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time when we train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Is the model converging faster than before?* Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "* *Does BN produce a better model?* Yes! The final model is also much better, with 54.0% accuracy instead of 47.6%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
    "* *How does BN affect training speed?* Although the model converged much faster, each epoch took about 12s instead of 8s, because of the extra computations required by the BN layers. But overall the training time (wall time) was shortened significantly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "*Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 20s 10ms/step - loss: 1.8996 - accuracy: 0.3196 - val_loss: 1.7557 - val_accuracy: 0.3836\n",
      "Epoch 2/100\n",
      "   6/1407 [..............................] - ETA: 14s - loss: 1.7794 - accuracy: 0.3281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Adit/codes/ml_101/handson-ml2-master/ml_env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6904 - accuracy: 0.3998 - val_loss: 1.6670 - val_accuracy: 0.4014\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5989 - accuracy: 0.4358 - val_loss: 1.6164 - val_accuracy: 0.4124\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5379 - accuracy: 0.4596 - val_loss: 1.6203 - val_accuracy: 0.4394\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4790 - accuracy: 0.4826 - val_loss: 1.5297 - val_accuracy: 0.4664\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4362 - accuracy: 0.4992 - val_loss: 1.5138 - val_accuracy: 0.4794\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3951 - accuracy: 0.5146 - val_loss: 1.5207 - val_accuracy: 0.4738\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3606 - accuracy: 0.5241 - val_loss: 1.4911 - val_accuracy: 0.4852\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3231 - accuracy: 0.5432 - val_loss: 1.5233 - val_accuracy: 0.4662\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.2931 - accuracy: 0.5522 - val_loss: 1.5348 - val_accuracy: 0.4724\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2680 - accuracy: 0.5616 - val_loss: 1.5294 - val_accuracy: 0.4974\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2403 - accuracy: 0.5719 - val_loss: 1.5349 - val_accuracy: 0.4870\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 1.2163 - accuracy: 0.5801 - val_loss: 1.4775 - val_accuracy: 0.5032\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1898 - accuracy: 0.5909 - val_loss: 1.5148 - val_accuracy: 0.4908\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1649 - accuracy: 0.6016 - val_loss: 1.5027 - val_accuracy: 0.5008\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 1.1483 - accuracy: 0.6074 - val_loss: 1.5139 - val_accuracy: 0.5122\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.1229 - accuracy: 0.6145 - val_loss: 1.5032 - val_accuracy: 0.4978\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.0939 - accuracy: 0.6268 - val_loss: 1.5275 - val_accuracy: 0.5022\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.0856 - accuracy: 0.6312 - val_loss: 1.5730 - val_accuracy: 0.5128\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.0678 - accuracy: 0.6333 - val_loss: 1.5017 - val_accuracy: 0.5108\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.0409 - accuracy: 0.6453 - val_loss: 1.5480 - val_accuracy: 0.5128\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0299 - accuracy: 0.6513 - val_loss: 1.5515 - val_accuracy: 0.5188\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.0083 - accuracy: 0.6601 - val_loss: 1.5757 - val_accuracy: 0.5016\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9916 - accuracy: 0.6638 - val_loss: 1.6136 - val_accuracy: 0.5120\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9711 - accuracy: 0.6690 - val_loss: 1.5787 - val_accuracy: 0.5136\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9505 - accuracy: 0.6796 - val_loss: 1.5762 - val_accuracy: 0.5100\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9442 - accuracy: 0.6808 - val_loss: 1.5967 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9229 - accuracy: 0.6892 - val_loss: 1.6140 - val_accuracy: 0.5092\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9143 - accuracy: 0.6937 - val_loss: 1.6065 - val_accuracy: 0.5144\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.8987 - accuracy: 0.6975 - val_loss: 1.6431 - val_accuracy: 0.5168\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.8897 - accuracy: 0.7023 - val_loss: 1.6038 - val_accuracy: 0.5134\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.8787 - accuracy: 0.7023 - val_loss: 1.6405 - val_accuracy: 0.5034\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.8614 - accuracy: 0.7112 - val_loss: 1.6783 - val_accuracy: 0.5084\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4775 - accuracy: 0.5032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.477542519569397, 0.5031999945640564]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time when we train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4775 - accuracy: 0.5032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.477542519569397, 0.5031999945640564]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 50.3% accuracy, which is not much better than the original model, and not as good as the model using batch normalization (55.0%). However, convergence was almost as fast as with the BN model, plus each epoch took only 7 seconds. So it's by far the fastest model to train so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.\n",
    "*Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 21s 11ms/step - loss: 1.8936 - accuracy: 0.3238 - val_loss: 1.7366 - val_accuracy: 0.3912\n",
      "Epoch 2/100\n",
      "   1/1407 [..............................] - ETA: 17s - loss: 1.8313 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Adit/codes/ml_101/handson-ml2-master/ml_env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6652 - accuracy: 0.4126 - val_loss: 1.6834 - val_accuracy: 0.4086\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5816 - accuracy: 0.4478 - val_loss: 1.6089 - val_accuracy: 0.4330\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5124 - accuracy: 0.4676 - val_loss: 1.6012 - val_accuracy: 0.4506\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4597 - accuracy: 0.4891 - val_loss: 1.5535 - val_accuracy: 0.4718\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4078 - accuracy: 0.5086 - val_loss: 1.5098 - val_accuracy: 0.4908\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3649 - accuracy: 0.5221 - val_loss: 1.5923 - val_accuracy: 0.4688\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3255 - accuracy: 0.5382 - val_loss: 1.5350 - val_accuracy: 0.4862\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.2921 - accuracy: 0.5508 - val_loss: 1.5327 - val_accuracy: 0.4736\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2558 - accuracy: 0.5609 - val_loss: 1.5631 - val_accuracy: 0.4880\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2259 - accuracy: 0.5764 - val_loss: 1.5921 - val_accuracy: 0.4906\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1954 - accuracy: 0.5861 - val_loss: 1.5319 - val_accuracy: 0.5014\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1641 - accuracy: 0.5978 - val_loss: 1.5468 - val_accuracy: 0.5006\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.1375 - accuracy: 0.6087 - val_loss: 1.5177 - val_accuracy: 0.5132\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.1077 - accuracy: 0.6156 - val_loss: 1.5945 - val_accuracy: 0.4904\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.0861 - accuracy: 0.6250 - val_loss: 1.6067 - val_accuracy: 0.5064\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.0588 - accuracy: 0.6374 - val_loss: 1.6404 - val_accuracy: 0.5094\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0400 - accuracy: 0.6440 - val_loss: 1.5973 - val_accuracy: 0.4972\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0179 - accuracy: 0.6505 - val_loss: 1.7249 - val_accuracy: 0.5122\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 0.9914 - accuracy: 0.6621 - val_loss: 1.7164 - val_accuracy: 0.4990\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 0.9757 - accuracy: 0.6666 - val_loss: 1.7095 - val_accuracy: 0.5016\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 0.9546 - accuracy: 0.6734 - val_loss: 1.6273 - val_accuracy: 0.5064\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.9291 - accuracy: 0.6826 - val_loss: 1.6415 - val_accuracy: 0.5022\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9185 - accuracy: 0.6869 - val_loss: 1.6986 - val_accuracy: 0.4976\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 37s 27ms/step - loss: 0.8983 - accuracy: 0.6934 - val_loss: 1.7779 - val_accuracy: 0.4986\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 0.8774 - accuracy: 0.7011 - val_loss: 1.7893 - val_accuracy: 0.5016\n",
      "157/157 [==============================] - 2s 7ms/step - loss: 1.5098 - accuracy: 0.4908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.509841799736023, 0.49079999327659607]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time when we train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches 49.0% accuracy on the validation set. That's very slightly better than without dropout (47.6%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the `MCAlphaDropout` class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with `MCAlphaDropout` dropout layers instead of `AlphaDropout` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's add a couple utility functions. \n",
    "\n",
    "The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. \n",
    "\n",
    "The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 6ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 5ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.491"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get no accuracy improvement in this case (we're still at 48.9% accuracy).\n",
    "\n",
    "So the best model we got in this exercise is the Batch Normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.\n",
    "*Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 14s 40ms/step - loss: nan - accuracy: 0.4889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 0.8319306969642639,\n",
       " 1.2571042350360326)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 10s 20ms/step - loss: 2.0527 - accuracy: 0.2809 - val_loss: 1.7548 - val_accuracy: 0.3686\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 12s 33ms/step - loss: 1.7595 - accuracy: 0.3752 - val_loss: 1.6625 - val_accuracy: 0.4142\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 16s 45ms/step - loss: 1.6250 - accuracy: 0.4218 - val_loss: 1.6169 - val_accuracy: 0.4280\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 14s 39ms/step - loss: 1.5487 - accuracy: 0.4498 - val_loss: 1.6379 - val_accuracy: 0.4264\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 13s 37ms/step - loss: 1.4979 - accuracy: 0.4690 - val_loss: 1.6878 - val_accuracy: 0.4318\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 13s 37ms/step - loss: 1.4575 - accuracy: 0.4836 - val_loss: 1.5933 - val_accuracy: 0.4450\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 11s 33ms/step - loss: 1.4136 - accuracy: 0.4982 - val_loss: 1.6650 - val_accuracy: 0.4426\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 14s 41ms/step - loss: 1.3516 - accuracy: 0.5208 - val_loss: 1.5386 - val_accuracy: 0.4708\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 12s 35ms/step - loss: 1.2777 - accuracy: 0.5464 - val_loss: 1.5581 - val_accuracy: 0.4796\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 12s 33ms/step - loss: 1.2036 - accuracy: 0.5707 - val_loss: 1.5200 - val_accuracy: 0.5024\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 9s 26ms/step - loss: 1.1363 - accuracy: 0.5944 - val_loss: 1.5800 - val_accuracy: 0.4920\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 9s 26ms/step - loss: 1.0696 - accuracy: 0.6183 - val_loss: 1.5150 - val_accuracy: 0.5058\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 10s 27ms/step - loss: 0.9982 - accuracy: 0.6419 - val_loss: 1.5430 - val_accuracy: 0.5162\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 14s 38ms/step - loss: 0.9348 - accuracy: 0.6661 - val_loss: 1.5662 - val_accuracy: 0.5222\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 14s 39ms/step - loss: 0.8932 - accuracy: 0.6813 - val_loss: 1.5936 - val_accuracy: 0.5232\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 47.6% to 52.0%). The batch normalized model reaches a slightly better performance (54%), but it's much slower to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
