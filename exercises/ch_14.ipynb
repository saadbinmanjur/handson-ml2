{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. High Accuracy CNN for MNIST\n",
    "_Exercise: Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model uses \n",
    "\n",
    "- 2 convolutional layers, \n",
    "\n",
    "- followed by 1 pooling layer, \n",
    "\n",
    "- then dropout 25%, \n",
    "\n",
    "- then a dense layer, \n",
    "\n",
    "- another dropout layer but with 50% dropout, \n",
    "\n",
    "- and finally the output layer. \n",
    "\n",
    "It reaches about 99.2% accuracy on the test set. This places this model roughly in the top 20% in the [MNIST Kaggle competition](https://www.kaggle.com/c/digit-recognizer/) (if we ignore the models with an accuracy greater than 99.79% which were most likely trained on the test set, as explained by Chris Deotte in [this post](https://www.kaggle.com/c/digit-recognizer/discussion/61480)). \n",
    "\n",
    "Can you do better? \n",
    "\n",
    "To reach 99.5 to 99.7% accuracy on the test set, you need to add \n",
    "\n",
    "- image augmentation, \n",
    "\n",
    "- batch norm, use a learning schedule such as 1-cycle, \n",
    "\n",
    "- and possibly create an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 140s 80ms/step - loss: 0.1922 - accuracy: 0.9428 - val_loss: 0.0500 - val_accuracy: 0.9860\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 133s 77ms/step - loss: 0.0812 - accuracy: 0.9751 - val_loss: 0.0444 - val_accuracy: 0.9884\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 127s 74ms/step - loss: 0.0637 - accuracy: 0.9806 - val_loss: 0.0356 - val_accuracy: 0.9914\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 129s 75ms/step - loss: 0.0474 - accuracy: 0.9853 - val_loss: 0.0342 - val_accuracy: 0.9912\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 124s 72ms/step - loss: 0.0429 - accuracy: 0.9867 - val_loss: 0.0328 - val_accuracy: 0.9908\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 125s 73ms/step - loss: 0.0382 - accuracy: 0.9877 - val_loss: 0.0333 - val_accuracy: 0.9924\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 126s 73ms/step - loss: 0.0319 - accuracy: 0.9900 - val_loss: 0.0301 - val_accuracy: 0.9924\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 125s 73ms/step - loss: 0.0298 - accuracy: 0.9899 - val_loss: 0.0364 - val_accuracy: 0.9914\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 126s 73ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.0376 - val_accuracy: 0.9916\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 125s 73ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.0392 - val_accuracy: 0.9912\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0314 - accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.031391628086566925, 0.9911999702453613]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training and testing\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.  Use transfer learning for large image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Use transfer learning for large image classification, going through these steps:_\n",
    "\n",
    "* _Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets)._\n",
    "* _Split it into a training set, a validation set, and a test set._\n",
    "* _Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation._\n",
    "* _Fine-tune a pretrained model on this dataset._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the dataset\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "\n",
    "# Define the classes\n",
    "num_classes = info.features['label'].num_classes\n",
    "\n",
    "# Define the training, validation, and test split sizes\n",
    "train_split = 0.7\n",
    "val_split = 0.15\n",
    "test_split = 0.15\n",
    "\n",
    "# Calculate the number of examples for each split\n",
    "num_examples = info.splits['train'].num_examples\n",
    "num_train_examples = int(train_split * num_examples)\n",
    "num_val_examples = int(val_split * num_examples)\n",
    "num_test_examples = num_examples - num_train_examples - num_val_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_dataset = dataset['train'].take(num_train_examples)\n",
    "val_dataset = dataset['train'].skip(num_train_examples).take(num_val_examples)\n",
    "test_dataset = dataset['train'].skip(num_train_examples + num_val_examples).take(num_test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))  # Resize image to expected input size for MobileNetV2\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)  # Preprocess the input image\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "train_dataset = train_dataset.map(preprocess_image)\n",
    "val_dataset = val_dataset.map(preprocess_image)\n",
    "test_dataset = test_dataset.map(preprocess_image)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained MobileNetV2 model\n",
    "pretrained_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the pretrained layers\n",
    "pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new classification head\n",
    "model = tf.keras.Sequential([\n",
    "    pretrained_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
