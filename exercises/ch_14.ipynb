{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. High Accuracy CNN for MNIST\n",
    "_Exercise: Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model uses \n",
    "\n",
    "- 2 convolutional layers, \n",
    "\n",
    "- followed by 1 pooling layer, \n",
    "\n",
    "- then dropout 25%, \n",
    "\n",
    "- then a dense layer, \n",
    "\n",
    "- another dropout layer but with 50% dropout, \n",
    "\n",
    "- and finally the output layer. \n",
    "\n",
    "It reaches about 99.2% accuracy on the test set. This places this model roughly in the top 20% in the [MNIST Kaggle competition](https://www.kaggle.com/c/digit-recognizer/) (if we ignore the models with an accuracy greater than 99.79% which were most likely trained on the test set, as explained by Chris Deotte in [this post](https://www.kaggle.com/c/digit-recognizer/discussion/61480)). \n",
    "\n",
    "Can you do better? \n",
    "\n",
    "To reach 99.5 to 99.7% accuracy on the test set, you need to add \n",
    "\n",
    "- image augmentation, \n",
    "\n",
    "- batch norm, use a learning schedule such as 1-cycle, \n",
    "\n",
    "- and possibly create an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 140s 80ms/step - loss: 0.1922 - accuracy: 0.9428 - val_loss: 0.0500 - val_accuracy: 0.9860\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 133s 77ms/step - loss: 0.0812 - accuracy: 0.9751 - val_loss: 0.0444 - val_accuracy: 0.9884\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 127s 74ms/step - loss: 0.0637 - accuracy: 0.9806 - val_loss: 0.0356 - val_accuracy: 0.9914\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 129s 75ms/step - loss: 0.0474 - accuracy: 0.9853 - val_loss: 0.0342 - val_accuracy: 0.9912\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 124s 72ms/step - loss: 0.0429 - accuracy: 0.9867 - val_loss: 0.0328 - val_accuracy: 0.9908\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 125s 73ms/step - loss: 0.0382 - accuracy: 0.9877 - val_loss: 0.0333 - val_accuracy: 0.9924\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 126s 73ms/step - loss: 0.0319 - accuracy: 0.9900 - val_loss: 0.0301 - val_accuracy: 0.9924\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 125s 73ms/step - loss: 0.0298 - accuracy: 0.9899 - val_loss: 0.0364 - val_accuracy: 0.9914\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 126s 73ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.0376 - val_accuracy: 0.9916\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 125s 73ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.0392 - val_accuracy: 0.9912\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0314 - accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.031391628086566925, 0.9911999702453613]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training and testing\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
